# -*- coding: utf-8 -*-
import os
import sys
import subprocess
import json
import re
import io
import time
import pandas as pd
from dotenv import load_dotenv
import streamlit as st
from PyPDF2 import PdfReader 
import google.generativeai as genai
from google.generativeai import types
from google.generativeai.types import GenerationConfig 
from typing import Dict, Any, Tuple
import altair as alt
from shadcn_style import SHADCN_CSS
from datetime import datetime

# Export Libraries
try:
    from openpyxl import Workbook
    from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
    EXCEL_AVAILABLE = True
except ImportError:
    EXCEL_AVAILABLE = False

try:
    from fpdf import FPDF
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False

try:
    from docx import Document as DocxDocument
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False


# --- 1. ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÅ‡∏•‡∏∞‡πÇ‡∏´‡∏•‡∏î Environment ---
load_dotenv()

# API Keys for all providers
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY', '').strip()
GROQ_API_KEY = os.getenv('GROQ_API_KEY', '').strip()
OPENROUTER_API_KEY = os.getenv('OPENROUTER_API_KEY', '').strip()

# &#128640; Multi-Provider AI Configuration
AI_PROVIDERS = {
    "Gemini (Google)": {
        "models": {
            "Gemini 2.0 Flash (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)": "gemini-2.0-flash",
            "Gemini 1.5 Flash (‡πÄ‡∏£‡πá‡∏ß)": "gemini-1.5-flash-latest",
            "Gemini 1.5 Pro (‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥)": "gemini-1.5-pro-latest",
        },
        "api_key": GEMINI_API_KEY,
    },
    "Groq (‡∏ü‡∏£‡∏µ+‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å)": {
        "models": {
            "Llama 3.3 70B (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)": "llama-3.3-70b-versatile",
            "Llama 3.1 8B (‡πÄ‡∏£‡πá‡∏ß)": "llama-3.1-8b-instant",
            "Mixtral 8x7B": "mixtral-8x7b-32768",
        },
        "api_key": GROQ_API_KEY,
    },
    "OpenRouter (‡∏´‡∏•‡∏≤‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ü‡∏£‡∏µ)": {
        "models": {
            "Llama 3.2 3B (‡∏ü‡∏£‡∏µ)": "meta-llama/llama-3.2-3b-instruct:free",
            "Mistral 7B (‡∏ü‡∏£‡∏µ)": "mistralai/mistral-7b-instruct:free",
            "Gemma 2 9B (‡∏ü‡∏£‡∏µ)": "google/gemma-2-9b-it:free",
        },
        "api_key": OPENROUTER_API_KEY,
    },
}

DEFAULT_PROVIDER = "Gemini (Google)"
DEFAULT_MODEL_NAME = "Gemini 2.0 Flash (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)"

# Legacy support - keep AVAILABLE_AI_MODELS for compatibility
AVAILABLE_AI_MODELS = AI_PROVIDERS[DEFAULT_PROVIDER]["models"]

# Check API availability for each provider
GEMINI_AVAILABLE = False
GROQ_AVAILABLE = False
OPENROUTER_AVAILABLE = False

try:
    if GEMINI_API_KEY and len(GEMINI_API_KEY) > 30:
        genai.configure(api_key=GEMINI_API_KEY)
        GEMINI_AVAILABLE = True
except Exception:
    GEMINI_AVAILABLE = False

if GROQ_API_KEY and len(GROQ_API_KEY) > 20:
    GROQ_AVAILABLE = True
    
if OPENROUTER_API_KEY and len(OPENROUTER_API_KEY) > 20:
    OPENROUTER_AVAILABLE = True

def render_user_manual():
    """Show User Manual"""
    with st.expander("&#128218; ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏°‡∏∑‡∏≠‡πÉ‡∏´‡∏°‡πà (‡∏Ñ‡∏•‡∏¥‡∏Å‡∏≠‡πà‡∏≤‡∏ô)", expanded=False):
        st.markdown('''
        **1. ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô &#128640;**
        - ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö (PDF, DOCX, TXT) ‡∏à‡∏≤‡∏Å‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á
        - ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏™‡∏Å‡∏±‡∏î‡πÇ‡∏à‡∏ó‡∏¢‡πå‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö
        
        **2. ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å AI (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç) &#129504;**
        - **Gemini**: ‡∏â‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÅ‡∏ï‡πà‡∏≠‡∏≤‡∏à‡∏ï‡∏¥‡∏î Limit (429)
        - **Groq**: ‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å & ‡∏ü‡∏£‡∏µ! (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏°‡∏∑‡πà‡∏≠ Gemini ‡πÄ‡∏ï‡πá‡∏°)
        - *‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà "‡πÄ‡∏•‡∏∑‡∏≠‡∏Å AI Provider" ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á*
        
        **3. ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏û‡∏¥‡πÄ‡∏®‡∏© &#10024;**
        - **Export**: ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Excel/PDF ‡∏ó‡∏µ‡πà‡πÅ‡∏ó‡πá‡∏ö Export
        - **‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö**: ‡πÉ‡∏´‡πâ AI ‡∏ä‡πà‡∏ß‡∏¢‡∏Ñ‡∏¥‡∏î‡πÇ‡∏à‡∏ó‡∏¢‡πå‡πÉ‡∏´‡∏°‡πà
        - **‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥**: ‡∏î‡∏π‡∏ú‡∏•‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡∏™‡∏∏‡∏î
        
        **4. ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Error 429 &#9888;**
        - ‡∏´‡∏≤‡∏Å AI ‡πÑ‡∏°‡πà‡∏ï‡∏≠‡∏ö (Quota Exceeded)
        - ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô Provider ‡πÄ‡∏õ‡πá‡∏ô **Groq** ‡∏´‡∏£‡∏∑‡∏≠ **OpenRouter** ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
        ''')


def render_top_navigation():
    """Show Top Navigation Bar (Settings & Manual)"""
    with st.container():
        col1, col2, col3, col4 = st.columns([1, 1, 1.5, 2])
        
        with col1:
            # Language
            st.button(
                t('language_btn'), 
                on_click=toggle_language,
                use_container_width=True,
                key='top_lang_toggle'
            )
            
        with col2:
            # Manual Popover/Expander
            with st.expander("üìö ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠", expanded=False):
                st.markdown(t('tip_1'))
                st.markdown(t('tip_2'))
                st.markdown("---")
                st.markdown(t('quota_warning'))

        with col3:
            # Provider Selector
            provider_options = list(AI_PROVIDERS.keys())
            current_idx = provider_options.index(st.session_state.selected_provider) if st.session_state.selected_provider in provider_options else 0
            
            new_provider = st.selectbox(
                "AI Provider",
                options=provider_options,
                index=current_idx,
                key='top_provider',
                label_visibility="collapsed"
            )
            
            if new_provider != st.session_state.selected_provider:
                st.session_state.selected_provider = new_provider
                # Reset model
                first_model = list(AI_PROVIDERS[new_provider]["models"].keys())[0]
                st.session_state.selected_model = first_model
                st.session_state.analysis_results = None
                st.rerun()

        with col4:
            # Model Selector
            model_options = list(AI_PROVIDERS[st.session_state.selected_provider]["models"].keys())
            current_midx = model_options.index(st.session_state.selected_model) if st.session_state.selected_model in model_options else 0
            
            new_model = st.selectbox(
                "Model",
                options=model_options,
                index=current_midx,
                key='top_model',
                label_visibility="collapsed"
            )
            
            if new_model != st.session_state.selected_model:
                st.session_state.selected_model = new_model
                st.session_state.analysis_results = None

def render_history_sidebar_v2():
    """Show History in Sidebar"""

    history = load_analysis_history()
    
    if not history:
        st.info("‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥")
        return

    # Show latest first
    for i, entry in enumerate(reversed(history)):
        timestamp = entry.get('timestamp', 'N/A')
        filename = entry.get('filename', 'Unknown')
        
        # Format nice timestamp
        try:
            dt = datetime.fromisoformat(timestamp)
            time_str = dt.strftime("%d/%m %H:%M")
        except:
            time_str = timestamp

        if st.button(f"üìÇ {filename}", key=f"hist_btn_{i}", use_container_width=True, help=f"‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà: {time_str}"):
            st.session_state.analysis_results = entry.get('results')
            st.session_state.question_texts = entry.get('question_texts') # Optional restore
            st.success(f"‡πÇ‡∏´‡∏•‡∏î‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥: {filename}")
            st.rerun()


# Initialize session states
if 'analysis_results' not in st.session_state:
    st.session_state.analysis_results = None
if 'last_uploaded_file_name' not in st.session_state:
    st.session_state.last_uploaded_file_name = None
if 'question_texts' not in st.session_state:
    st.session_state.question_texts = None
if 'language' not in st.session_state:
    st.session_state.language = 'th'
if 'custom_prompt' not in st.session_state:
    st.session_state.custom_prompt = ""
if 'selected_provider' not in st.session_state:
    st.session_state.selected_provider = DEFAULT_PROVIDER
if 'selected_model' not in st.session_state:
    st.session_state.selected_model = DEFAULT_MODEL_NAME

# --- Translation Dictionary ---
TRANSLATIONS = {
    'th': {
        # Header
        'app_title': '&#128640; ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö',
        'app_subtitle': '&#10024; ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏î‡πâ‡∏ß‡∏¢ <strong style="color: #667eea;">Gemini AI</strong> ‡∏ï‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡πÅ‡∏Å‡∏ô‡∏Å‡∏•‡∏≤‡∏á‡∏Ø ‡πÅ‡∏•‡∏∞ <strong style="color: #764ba2;">Bloom\'s Taxonomy</strong>',
        
        # Sidebar
        'sidebar_title': '‚öôÔ∏è ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ & ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ AI',
        'ai_connected': '&#9989; ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ AI ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à',
        'ai_not_connected': '&#10060; ‡πÑ‡∏°‡πà‡∏û‡∏ö API Key (GEMINI_API_KEY) - ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô `.env`',
        'model_used': '**‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô**',
        'batch_analysis': 'Batch Analysis',
        'tips_title': '&#128161; ‡πÄ‡∏Ñ‡∏•‡πá‡∏î‡∏•‡∏±‡∏ö',
        'tip_1': '- ‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏•‡πå **PDF** ‡∏´‡∏£‡∏∑‡∏≠ **TXT**',
        'tip_2': '- ‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏£‡∏°‡∏µ **‡πÄ‡∏•‡∏Ç‡∏Ç‡πâ‡∏≠** (‡πÄ‡∏ä‡πà‡∏ô 1., 2.) ‡πÅ‡∏•‡∏∞ **‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å** (‡πÄ‡∏ä‡πà‡∏ô ‡∏Å., ‡∏Ç.)',
        'api_warning': '‡πÇ‡∏õ‡∏£‡∏î‡∏ó‡∏£‡∏≤‡∏ö: ‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ GEMINI_API_KEY ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå .env ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå',
        
        # Custom Prompt
        'custom_prompt_title': 'üìù Custom Prompt (‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö)',
        'custom_prompt_label': '‡∏Å‡∏£‡∏≠‡∏Å Prompt ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡πÅ‡∏ó‡∏ô Prompt.txt:',
        'custom_prompt_placeholder': '‡∏õ‡∏•‡πà‡∏≠‡∏¢‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ Prompt ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå Prompt.txt...\n\n‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏£‡∏≠‡∏Å Prompt ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà ‡πÄ‡∏ä‡πà‡∏ô:\n\n‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏ô‡∏µ‡πâ‡∏ï‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å Bloom\'s Taxonomy ‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û...',
        'custom_prompt_active': '&#10024; ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏ä‡πâ Custom Prompt',
        'custom_prompt_default': '&#128196; ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏ä‡πâ Prompt ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå',
        
        # Step 1
        'step1_title': '1Ô∏è‚É£ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö (Batch Analysis)',
        'file_uploader_label': 'üìÅ ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö **(.PDF ‡∏´‡∏£‡∏∑‡∏≠ .TXT)**',
        'reading_file': '‚è≥ **‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö:**',
        'from_file': '‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå',
        'extracting': '‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°...',
        'no_questions_found': '&#10060; **‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö** ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÑ‡∏ü‡∏•‡πå (‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏•‡∏Ç‡∏Ç‡πâ‡∏≠/‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å ‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ)',
        'file_tip': '&#128161; **‡πÄ‡∏Ñ‡∏•‡πá‡∏î‡∏•‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÑ‡∏ü‡∏•‡πå:** ‡πÑ‡∏ü‡∏•‡πå‡∏Ñ‡∏ß‡∏£‡∏°‡∏µ **‡πÄ‡∏•‡∏Ç‡∏Ç‡πâ‡∏≠** ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô (‡πÄ‡∏ä‡πà‡∏ô 1., 2., 3.) ‡πÅ‡∏•‡∏∞‡∏°‡∏µ **‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å** (‡πÄ‡∏ä‡πà‡∏ô ‡∏Å., ‡∏Ç., ‡∏Ñ., ‡∏á.)',
        'file_read_error': '&#10060; **‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå:**',
        'extracted_questions': '&#9989; ‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡πÑ‡∏î‡πâ‡πÅ‡∏•‡πâ‡∏ß **{count} ‡∏Ç‡πâ‡∏≠** ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå `{filename}`',
        
        # Step 2
        'step2_title': '2Ô∏è‚É£ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô &#128640;',
        'start_analysis_btn': '&#128640; **‡∏Å‡∏î‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢ AI**',
        'api_not_ready': 'Key ‡πÑ‡∏°‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤',
        'starting_analysis': '&#128640; **‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢ AI...**',
        'preparing_analysis': '‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö {count} ‡∏Ç‡πâ‡∏≠ ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ `{model}`',
        'analyzing_question': '&#129302; ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà {num}...',
        'analysis_progress': '‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö {current}/{total} ‡∏Ç‡πâ‡∏≠...',
        'analysis_complete': 'üéâ **‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!**',
        
        # Step 3 - Results
        'step3_title': '3Ô∏è‚É£ ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö üìù',
        'tab_summary': 'üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô & ‡πÄ‡∏Å‡∏ì‡∏ë‡πå Bloom',
        'tab_details': 'üìù ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏£‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠',
        'summary_title': 'üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö',
        'good_questions': '&#9989; ‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏î‡∏µ',
        'needs_improvement': '&#9888; ‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á',
        'total_questions': 'üìù ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î',
        'analyzed_success': '&#129302; ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à',
        'bloom_criteria_title': '&#128161; ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î (Bloom)',
        'bloom_low': '‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡∏ï‡πà‡∏≥ (‡∏à‡∏≥/‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à)',
        'bloom_mid': '‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡∏Å‡∏•‡∏≤‡∏á (‡πÉ‡∏ä‡πâ/‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå)',
        'bloom_high': '‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡∏™‡∏π‡∏á (‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô/‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏£‡∏Ñ‡πå)',
        'target': '‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢',
        'unidentified_bloom': '**‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ:**',
        'bloom_distribution': 'üìà ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏£‡∏∞‡∏î‡∏±‡∏ö Bloom\'s Taxonomy',
        'bloom_table_title': '**‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö Bloom**',
        'details_title': 'üìù ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠',
        'click_detail': '### üîé ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå (10 Fields) ‡∏£‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠',
        'question_num': '‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà',
        'quality': '‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö',
        'bloom_level': '‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î',
        'curriculum': '‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£',
        'answer': '‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö',
        'reasoning': '‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡πÇ‡∏î‡∏¢‡∏¢‡πà‡∏≠',
        'suggestion': '‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞',
        'full_question': '**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏ï‡πá‡∏°:**',
        'good': '&#9989; ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏î‡∏µ',
        'improve': '&#10060; ‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á/‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß',
        'difficulty': '‚öñÔ∏è ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å:',
        'correct_answer': '&#9989; ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:',
        'curriculum_indicator': '**&#128218; ‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£:**',
        'bloom_reason': '**&#129504; ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏Ç‡∏≠‡∏á‡∏£‡∏∞‡∏î‡∏±‡∏ö Bloom/‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û:**',
        'answer_analysis_title': '‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡∏•‡∏ß‡∏á',
        'correct_analysis': '**&#9989; ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å:**',
        'distractor_analysis': '**&#10060; ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏•‡∏ß‡∏á (Distractors):**',
        'why_good_distractor': '**&#128161; ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏ß‡∏•‡∏ß‡∏á‡∏î‡∏µ:**',
        'improvement_suggestion': '**üîß ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á:**',
        
        # Quota Warning
        'quota_warning': '&#9888; **‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î Free Tier:** 20 requests/‡∏ß‡∏±‡∏ô ‡∏´‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÇ‡∏Ñ‡∏ß‡∏ï‡πâ‡∏≤ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏≠ 24 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á ‡∏´‡∏£‡∏∑‡∏≠‡∏≠‡∏±‡∏õ‡πÄ‡∏Å‡∏£‡∏î‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô',
        
        # Language
        'language_btn': 'üåê English',
    },
    'en': {
        # Header
        'app_title': '&#128640; Exam Quality Analysis Tool',
        'app_subtitle': '&#10024; Automatic exam analysis with <strong style="color: #667eea;">Gemini AI</strong> based on Core Curriculum and <strong style="color: #764ba2;">Bloom\'s Taxonomy</strong>',
        
        # Sidebar
        'sidebar_title': '‚öôÔ∏è Settings & AI Status',
        'ai_connected': '&#9989; AI Connected Successfully',
        'ai_not_connected': '&#10060; API Key not found (GEMINI_API_KEY) - Please set in `.env`',
        'model_used': '**Model Used**',
        'batch_analysis': 'Batch Analysis',
        'tips_title': '&#128161; Tips',
        'tip_1': '- Use **PDF** or **TXT** files',
        'tip_2': '- Questions should have **numbers** (e.g., 1., 2.) and **choices** (e.g., A., B.)',
        'api_warning': 'Note: You must set GEMINI_API_KEY in .env file to use the analysis feature',
        
        # Custom Prompt
        'custom_prompt_title': 'üìù Custom Prompt (Optional)',
        'custom_prompt_label': 'Enter custom prompt to use instead of Prompt.txt:',
        'custom_prompt_placeholder': 'Leave empty to use default Prompt.txt...\n\nOr enter your custom prompt here, e.g.:\n\nAnalyze this exam question according to Bloom\'s Taxonomy and rate its quality...',
        'custom_prompt_active': '&#10024; Using Custom Prompt',
        'custom_prompt_default': '&#128196; Using Default Prompt File',
        
        # Step 1
        'step1_title': '1Ô∏è‚É£ Upload Exam File (Batch Analysis)',
        'file_uploader_label': 'üìÅ Select exam file **(.PDF or .TXT)**',
        'reading_file': '‚è≥ **Reading and extracting questions:**',
        'from_file': 'from file',
        'extracting': 'Extracting text...',
        'no_questions_found': '&#10060; **No questions found** Please check the file format (no question numbers/choices or format too complex)',
        'file_tip': '&#128161; **File preparation tip:** File should have clear **question numbers** (e.g., 1., 2., 3.) and **choices** (e.g., A., B., C., D.)',
        'file_read_error': '&#10060; **Error reading file:**',
        'extracted_questions': '&#9989; Extracted **{count} questions** from file `{filename}`',
        
        # Step 2
        'step2_title': '2Ô∏è‚É£ Start Analysis & Generate Report &#128640;',
        'start_analysis_btn': '&#128640; **Click here to start AI analysis**',
        'api_not_ready': 'API Key not ready. Please check settings',
        'starting_analysis': '&#128640; **Starting AI analysis...**',
        'preparing_analysis': '‚è≥ Preparing to analyze {count} questions using `{model}`',
        'analyzing_question': '&#129302; Analyzing question {num}...',
        'analysis_progress': 'Analyzing question {current}/{total}...',
        'analysis_complete': 'üéâ **Analysis Complete!**',
        
        # Step 3 - Results
        'step3_title': '3Ô∏è‚É£ Exam Analysis Results üìù',
        'tab_summary': 'üìä Summary & Bloom Criteria',
        'tab_details': 'üìù Question Details',
        'summary_title': 'üìä Overall Exam Quality Summary',
        'good_questions': '&#9989; Good Quality Questions',
        'needs_improvement': '&#9888; Needs Improvement',
        'total_questions': 'üìù Total Questions',
        'analyzed_success': '&#129302; Successfully Analyzed',
        'bloom_criteria_title': '&#128161; Bloom\'s Taxonomy Distribution Criteria',
        'bloom_low': 'Lower Order (Remember/Understand)',
        'bloom_mid': 'Middle Order (Apply/Analyze)',
        'bloom_high': 'Higher Order (Evaluate/Create)',
        'target': 'Target',
        'unidentified_bloom': '**Questions with unidentified Bloom level:**',
        'bloom_distribution': 'üìà Bloom\'s Taxonomy Distribution',
        'bloom_table_title': '**Summary Table: Questions by Bloom Level**',
        'details_title': 'üìù Detailed Analysis per Question',
        'click_detail': '### üîé Click to view detailed analysis (10 Fields) per question',
        'question_num': 'Q#',
        'quality': 'Quality',
        'bloom_level': 'Bloom Level',
        'curriculum': 'Curriculum Standard',
        'answer': 'Answer',
        'reasoning': 'Brief Reasoning',
        'suggestion': 'Suggestion',
        'full_question': '**Full Question:**',
        'good': '&#9989; Good',
        'improve': '&#10060; Needs Improvement/Failed',
        'difficulty': '‚öñÔ∏è Difficulty:',
        'correct_answer': '&#9989; Answer:',
        'curriculum_indicator': '**&#128218; Curriculum Indicator:**',
        'bloom_reason': '**&#129504; Bloom Level/Quality Reasoning:**',
        'answer_analysis_title': 'Answer & Distractor Analysis',
        'correct_analysis': '**&#9989; Correct Answer Analysis:**',
        'distractor_analysis': '**&#10060; Distractor Analysis:**',
        'why_good_distractor': '**&#128161; Why Good Distractors:**',
        'improvement_suggestion': '**üîß Improvement Suggestion:**',
        
        # Quota Warning
        'quota_warning': '&#9888; **Free Tier Limit:** 20 requests/day. If exceeded, please wait 24 hours or upgrade your plan.',
        
        # Language
        'language_btn': 'üåê ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢',
    }
}

def t(key):
    """Get translation for current language"""
    lang = st.session_state.get('language', 'th')
    return TRANSLATIONS.get(lang, TRANSLATIONS['th']).get(key, key)


# --- 2. ‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î Prompt Template ---
def load_prompts() -> Tuple[str, str]:
    """‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤ Prompt Template ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå 'Prompt.txt' ‡πÅ‡∏•‡∏∞‡πÅ‡∏¢‡∏Å‡πÄ‡∏õ‡πá‡∏ô System/Chat"""
    prompt_path = os.path.join(os.path.dirname(__file__), 'Prompt.txt')
    try:
        with open(prompt_path, 'r', encoding='utf-8') as f:
            full_content = f.read()
            
            # 1. System Instruction
            system_match = full_content.split("# --- END_SYSTEM_INSTRUCTION_ANALYSIS_MODE ---")[0]
            SYSTEM_INSTRUCTION_PROMPT = system_match.strip() if system_match else "You are a test expert for Thai curriculum. Analyze the question and return a JSON object."
            
            # 2. Few-Shot Template 
            chat_match = full_content.split("# --- CHAT_PROMPT ---")
            
            if len(chat_match) > 1:
                # ‡πÅ‡∏¢‡∏Å‡∏™‡πà‡∏ß‡∏ô Prompt Template ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
                template_content = chat_match[1].split("# --- CHAT_PROMPT_END ---")[0].strip()
                # ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° {user_query} ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ó‡πâ‡∏≤‡∏¢
                FEW_SHOT_PROMPT_TEMPLATE = template_content + "\n{user_query}"
            else:
                FEW_SHOT_PROMPT_TEMPLATE = "Analyze the following question/text and return the JSON object: {user_query}"

            return SYSTEM_INSTRUCTION_PROMPT, FEW_SHOT_PROMPT_TEMPLATE
    except FileNotFoundError:
        return (
            "# Error: Prompt file not found. Fallback to default instruction.", 
            "Analyze the following question/text and return the JSON object: {user_query}"
        )

SYSTEM_INSTRUCTION_PROMPT, FEW_SHOT_PROMPT_TEMPLATE = load_prompts()


# --- 3. Helper Functions ---

BLOOM_COLORS = {
    "REMEMBER": "#6495ED",  
    "UNDERSTAND": "#40E0D0",
    "APPLY": "#50C878",     
    "ANALYZE": "#8FBC8F",   
    "EVALUATE": "#ADFF2F",  
    "CREATE": "#FFD700",    
    "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏": "#A9A9A9",
    "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏î‡πâ": "#A9A9A9"
}

def get_bloom_color(level):
    # Returns Hex color for Bloom's Taxonomy level
    if not level:
        return BLOOM_COLORS['‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏']
    
    level_upper = level.strip().upper()
    for key, color in BLOOM_COLORS.items():
        if key in level_upper:
            return color
    return BLOOM_COLORS['‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏'] # Fallback

def get_text_color_for_bloom(level):
    """‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏™‡∏µ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏î‡∏Å‡∏±‡∏ö‡∏™‡∏µ‡∏û‡∏∑‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏á (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏°‡∏≠‡∏á‡πÄ‡∏´‡πá‡∏ô‡πÑ‡∏î‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô)"""
    level_upper = level.strip().upper()
    if level_upper in ['EVALUATE', 'CREATE']: # ‡∏™‡∏µ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏á/‡∏ó‡∏≠‡∏á/‡∏™‡∏ß‡πà‡∏≤‡∏á ‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏™‡∏µ‡∏î‡∏≥
        return 'black' 
    return 'white' # ‡∏™‡∏µ‡πÄ‡∏Ç‡πâ‡∏°‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏™‡∏µ‡∏Ç‡∏≤‡∏ß


def extract_text_from_pdf(pdf_reader):
    """‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏ PdfReader"""
    text = ""
    for page in pdf_reader.pages:
        try:
            page_text = page.get_text() if hasattr(page, 'get_text') else page.extract_text()
            text += (page_text or "") + "\n"
        except Exception:
            text += "\n"
    return text.strip()


def clean_and_normalize(text):
    """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏•‡∏Ç‡πÑ‡∏ó‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏•‡∏Ç‡∏≠‡∏≤‡∏£‡∏ö‡∏¥‡∏Å"""
    if not text: return ""
    # 1. ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏•‡∏Ç‡πÑ‡∏ó‡∏¢
    thai_digits = "‡πê‡πë‡πí‡πì‡πî‡πï‡πñ‡πó‡πò‡πô"
    for i, digit in enumerate(thai_digits):
        text = text.replace(digit, str(i))
    
    # 2. ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏û‡∏¥‡πÄ‡∏®‡∏©‡πÅ‡∏•‡∏∞‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á
    text = re.sub(r'[ \t]+', ' ', text) 
    text = text.replace('\r', '')
    
    # 3. ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ö‡∏à‡∏∏‡∏î (‡πÄ‡∏ä‡πà‡∏ô ‡∏Å . -> ‡∏Å.)
    text = re.sub(r'([‡∏Å-‡∏áA-D])\s*\.', r'\1.', text)
    
    # 4. ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏Ç‡∏Ç‡πâ‡∏≠‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ö‡∏à‡∏∏‡∏î (‡πÄ‡∏ä‡πà‡∏ô 1 . -> 1.)
    text = re.sub(r'(\d+)\s*\.', r'\1.', text)
    
    # NEW: Insert newline before potential question start if preceded by punctuation or space
    # Matches: "text. 2. text" -> "text.\n2. text"
    # Matches: "text (2) text" -> "text\n(2) text"
    text = re.sub(r'(\s+)(\(?\d+[\.\)])\s', r'\n\2 ', text)
    
    # 5. ‡∏•‡∏ö‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏Å‡∏±‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î
    text = re.sub(r'\n{2,}', '\n', text)
    
    lines = [line.strip() for line in text.split('\n')]
    return '\n'.join(lines)


def extract_questions_with_ai(raw_text):
    """
    Fallback: ‡πÉ‡∏´‡πâ AI ‡∏ä‡πà‡∏ß‡∏¢‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡πÄ‡∏°‡∏∑‡πà‡∏≠ Regex ‡πÄ‡∏≠‡∏≤‡πÑ‡∏°‡πà‡∏≠‡∏¢‡∏π‡πà
    """
    if not GEMINI_AVAILABLE:
        return []

    try:
        model = genai.GenerativeModel("gemini-1.5-flash-latest") # Use Flash for speed/cost
        
        prompt = f"""
        You are an expert exam parser. 
        Please extract all exam questions from the following text and return them as a JSON list of strings.
        
        Rules:
        1. Capture the full question text including the question number and all options (e.g. "1. Question... A. Opt...").
        2. Do not change the original text, just split it correctly.
        3. If there are no clear questions, return an empty list.
        4. Return ONLY raw JSON Array.

        Text to parse:
        {raw_text[:20000]} 
        """
        # Limit text to 20k chars to avoid token limits on fallback
        
        response = model.generate_content(prompt, generation_config={"response_mime_type": "application/json"})
        questions = json.loads(response.text)
        
        if isinstance(questions, list):
            return [str(q).strip() for q in questions]
        else:
            return []
            
    except Exception as e:
        print(f"AI Extraction Failed: {e}")
        return []

def extract_questions(raw_text):
    """
    ‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠ (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÉ‡∏´‡πâ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö: 1., 1), (1), ‡∏Ç‡πâ‡∏≠ 1, ‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà 1)
    """
    # 1. ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    text = re.split(r"={10,}\s*‡πÄ‡∏â‡∏•‡∏¢\s*={10,}", raw_text, flags=re.DOTALL | re.IGNORECASE)[0]
    cleaned_text = clean_and_normalize(text)
    
    # 2. Regex ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏ö‡πÄ‡∏•‡∏Ç‡∏Ç‡πâ‡∏≠
    # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö: "1.", "1)", "(1)", "‡∏Ç‡πâ‡∏≠ 1", "‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà 1", "‡∏Ç‡πâ‡∏≠ ‡πë", (‡πÅ‡∏•‡∏∞‡πÅ‡∏ö‡∏ö‡∏°‡∏µ space)
    question_pattern = r'(?:^|\n)\s*((?:‡∏Ç‡πâ‡∏≠\s*\d+|‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà\s*\d+|\d+\.|(?:\(?\d+\)))[\.\s]+)'
    
    chunks = re.split(question_pattern, cleaned_text)
    
    questions = []
    
    # Skip preamble
    start_idx = 0
    if len(chunks) > 0 and not chunks[0].strip():
        start_idx = 1
    elif len(chunks) > 0 and not re.match(r'(?:‡∏Ç‡πâ‡∏≠\s*\d+|‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà\s*\d+|\d+\.|(?:\(?\d+\)))', chunks[0].strip()):
        start_idx = 1 # Skip likely header

    for i in range(start_idx, len(chunks), 2):
        if i+1 < len(chunks):
            delim = chunks[i]
            content = chunks[i+1]
            full_q = delim + content
            questions.append(full_q.strip())

    # 3. ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á (Validation ‡πÅ‡∏ö‡∏ö‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô)
    valid_questions = []
    for q in questions:
        q = q.strip()
        if len(q) < 5: continue 
        
        has_std_options = len(re.findall(r'[‡∏Å-‡∏áA-D]\.', q)) >= 2
        
        if has_std_options:
            q_formatted = re.sub(r'(\s+)([‡∏Å-‡∏áA-D]\.)', r'\n\2', q)
            valid_questions.append(q_formatted)
        else:
            if len(q) > 10: 
                valid_questions.append(q)
    
    # --- FALLBACK TO AI ---
    # ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÄ‡∏•‡∏¢ ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏à‡∏≠‡∏ô‡πâ‡∏≠‡∏¢‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (‡πÄ‡∏ä‡πà‡∏ô ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß 5000 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ ‡πÅ‡∏ï‡πà‡πÄ‡∏à‡∏≠ 0 ‡∏Ç‡πâ‡∏≠)
    # ‡∏´‡∏£‡∏∑‡∏≠‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠ < 3 ‡∏Ç‡πâ‡∏≠ ‡πÅ‡∏ï‡πà‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏°‡∏≤‡∏Å ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ AI ‡∏ä‡πà‡∏ß‡∏¢
    is_suspiciously_low = len(valid_questions) == 0 or (len(valid_questions) < 3 and len(raw_text) > 500)
    
    if is_suspiciously_low and GEMINI_AVAILABLE:
        st.toast("&#9888; ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÉ‡∏ä‡πâ AI ‡∏ä‡πà‡∏ß‡∏¢‡πÅ‡∏Å‡∏∞‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö...", icon="&#129302;")
        ai_extracted = extract_questions_with_ai(raw_text)
        if len(ai_extracted) > len(valid_questions):
             return ai_extracted

    return valid_questions


def analyze_with_gemini(question_text, question_id=1):
    """‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ Gemini API ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö (10 Fields Logic)"""
    if not GEMINI_AVAILABLE:
        return {
            "bloom_level": "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏", "reasoning": "‡πÑ‡∏°‡πà‡∏û‡∏ö API Key",
            "difficulty": "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏", "curriculum_standard": "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏",
            "correct_option": "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏", "correct_option_analysis": "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ AI",
            "distractor_analysis": "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ AI", "why_good_distractor": "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ AI",
            "is_good_question": False, "improvement_suggestion": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏î‡πâ: ‡πÑ‡∏°‡πà‡∏û‡∏ö API Key"
        } 

    # Check for custom prompt
    custom_prompt = st.session_state.get('custom_prompt', '').strip()
    
    if custom_prompt:
        # Use custom prompt as system instruction
        system_instruction = custom_prompt
        prompt_template = "{user_query}"  # Simple template for custom prompt
    else:
        # Use default prompts from Prompt.txt
        system_instruction = SYSTEM_INSTRUCTION_PROMPT
        prompt_template = FEW_SHOT_PROMPT_TEMPLATE

    # ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
    selected_model_name = st.session_state.get('selected_model', DEFAULT_MODEL_NAME)
    model_id = AVAILABLE_AI_MODELS.get(selected_model_name, "gemini-2.0-flash")
    
    model = genai.GenerativeModel(
        model_id, 
        system_instruction=system_instruction
    )
    
    question_text_formatted = f"‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà {question_id}:\n{question_text}"
    
    # ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Prompt ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
    full_prompt = prompt_template.format(user_query=question_text_formatted) 
    
    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î JSON Schema (10 Fields)
    json_schema = {
        "type": "object",
        "properties": {
            "bloom_level": {"type": "string"},
            "reasoning": {"type": "string"},
            "difficulty": {"type": "string"},
            "curriculum_standard": {"type": "string"},
            "correct_option": {"type": "string"},
            "correct_option_analysis": {"type": "string"},
            "distractor_analysis": {"type": "string"},
            "why_good_distractor": {"type": "string"},
            "is_good_question": {"type": "boolean"},
            "improvement_suggestion": {"type": "string"}
        },
        "required": [
            "bloom_level", "reasoning", "difficulty", "curriculum_standard",
            "correct_option", "correct_option_analysis", "distractor_analysis",
            "why_good_distractor", "is_good_question", "improvement_suggestion"
        ]
    }

    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Config (‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö JSON ‡πÅ‡∏•‡∏∞‡∏•‡∏î Temperature)
    config = GenerationConfig( 
        response_mime_type="application/json", 
        response_schema=json_schema, 
        temperature=0.2  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô
    )
    
    last_error_message = ""
    max_retries = 5  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô retry

    for attempt in range(max_retries):
        # Exponential backoff with jitter
        if attempt > 0:
            base_delay = min(60, (2 ** attempt) + (attempt * 2))  # 2, 6, 12, 22, 38 seconds
            time.sleep(base_delay)
            
        try:
            response = model.generate_content(
                full_prompt, 
                generation_config=config, 
            )
            raw_text = response.text.strip()
        
            # Hardened JSON Cleaning/Extraction
            cleaned_json = re.sub(r'^```(?:json)?\s*|```$', '', raw_text, flags=re.MULTILINE | re.DOTALL).strip()
            start_brace = cleaned_json.find('{')
            end_brace = cleaned_json.rfind('}')
            
            if start_brace != -1 and end_brace != -1 and end_brace > start_brace:
                cleaned_json = cleaned_json[start_brace:end_brace+1].strip()
            else:
                raise ValueError("Could not find valid JSON structure (missing { or }).") 
            
            # ‡πÇ‡∏´‡∏•‡∏î JSON
            analysis = json.loads(cleaned_json)
            
            # Data sanitation via shared function
            final_analysis = sanitize_analysis(analysis)
            return final_analysis

        except (json.JSONDecodeError, ValueError, KeyError) as e:
            last_error_message = f"‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• JSON/Key: {type(e).__name__}: {str(e)}"
            if attempt < max_retries - 1:
                continue  # ‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà (Retry)
            
        except Exception as e:
            error_str = str(e).lower()
            
            # Handle Rate Limit / Quota Exceeded (429 Error)
            is_rate_limit = any([
                "429" in str(e),
                "quota" in error_str,
                "resourceexhausted" in error_str,
                "rate" in error_str and "limit" in error_str,
                "too many requests" in error_str
            ])
            
            if is_rate_limit:
                # Calculate progressive delay
                if attempt < max_retries - 1:
                    retry_delay = min(120, 15 * (attempt + 1))  # 15, 30, 45, 60, 75 seconds
                    last_error_message = f"‚è≥ Rate Limit: ‡∏£‡∏≠ {retry_delay} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ ‡πÅ‡∏•‡πâ‡∏ß‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà... (‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà {attempt + 1}/{max_retries})"
                    time.sleep(retry_delay)
                    continue
                else:
                    last_error_message = f"&#10060; Quota Exceeded: ‡∏Ñ‡∏∏‡∏ì‡πÉ‡∏ä‡πâ‡πÇ‡∏Ñ‡∏ß‡∏ï‡πâ‡∏≤ API ‡∏Ñ‡∏£‡∏ö‡πÅ‡∏•‡πâ‡∏ß ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏≠‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á API Key ‡πÉ‡∏´‡∏°‡πà"
                    break
            else:
                last_error_message = f"‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {type(e).__name__}: {str(e)}"
                if attempt < max_retries - 1:
                    continue

    # Fallback ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢: ‡∏´‡∏≤‡∏Å AI ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢
    return {
        "bloom_level": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏î‡πâ", "reasoning": "AI ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß",
        "difficulty": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÑ‡∏î‡πâ", "curriculum_standard": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏î‡πâ",
        "correct_option": "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏", "correct_option_analysis": "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏",
        "distractor_analysis": "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏", "why_good_distractor": "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏",
        "is_good_question": False, 
        "improvement_suggestion": f"**‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå**: {last_error_message}"
    }


def analyze_with_groq(question_text, question_id=1):
    """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏ú‡πà‡∏≤‡∏ô Groq API (Llama, Mixtral) [Robust]"""
    from groq import Groq
    
    if not GROQ_AVAILABLE:
        return create_error_response("‡πÑ‡∏°‡πà‡∏û‡∏ö GROQ_API_KEY")
    
    client = Groq(api_key=GROQ_API_KEY)
    
    # Get selected model
    selected_model = st.session_state.get('selected_model', 'Llama 3.3 70B (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)')
    model_id = AI_PROVIDERS["Groq (‡∏ü‡∏£‡∏µ+‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å)"]["models"].get(selected_model, "llama-3.3-70b-versatile")
    
    # Build prompt
    custom_prompt = st.session_state.get('custom_prompt', '').strip()
    system_prompt = custom_prompt if custom_prompt else SYSTEM_INSTRUCTION_PROMPT
    
    user_message = f"""‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà {question_id}:
{question_text}

‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON ‡∏ó‡∏µ‡πà‡∏°‡∏µ keys: bloom_level, reasoning, difficulty, curriculum_standard, correct_option, correct_option_analysis, distractor_analysis, why_good_distractor, is_good_question (boolean), improvement_suggestion"""
    
    max_retries = 3
    last_error = ""
    
    for attempt in range(max_retries):
        if attempt > 0:
            time.sleep(attempt * 2)
            
        try:
            response = client.chat.completions.create(
                model=model_id,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message}
                ],
                temperature=0.2,
                max_tokens=2000,
                response_format={"type": "json_object"}
            )
            
            raw_text = response.choices[0].message.content
            analysis = json.loads(raw_text)
            return sanitize_analysis(analysis)
            
        except Exception as e:
            error_str = str(e).lower()
            if "429" in error_str or "rate limit" in error_str:
                last_error = f"Rate Limit (‡∏£‡∏≠ {attempt*2}s)"
                time.sleep(5) # Extra wait for rate limit
            else:
                last_error = str(e)
            
    return create_error_response(f"Groq Error (Max Retries): {last_error}")



def analyze_with_openrouter(question_text, question_id=1):
    """‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏ú‡πà‡∏≤‡∏ô OpenRouter API [Robust]"""
    import openai
    
    if not OPENROUTER_AVAILABLE:
        return create_error_response("‡πÑ‡∏°‡πà‡∏û‡∏ö OPENROUTER_API_KEY")
    
    client = openai.OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=OPENROUTER_API_KEY
    )
    
    # Get selected model
    selected_model = st.session_state.get('selected_model', 'Llama 3.2 3B (‡∏ü‡∏£‡∏µ)')
    model_id = AI_PROVIDERS["OpenRouter (‡∏´‡∏•‡∏≤‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ü‡∏£‡∏µ)"]["models"].get(selected_model, "meta-llama/llama-3.2-3b-instruct:free")
    
    # Build prompt  
    custom_prompt = st.session_state.get('custom_prompt', '').strip()
    system_prompt = custom_prompt if custom_prompt else SYSTEM_INSTRUCTION_PROMPT
    
    user_message = f"""‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà {question_id}:
{question_text}

‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON ‡∏ó‡∏µ‡πà‡∏°‡∏µ keys: bloom_level, reasoning, difficulty, curriculum_standard, correct_option, correct_option_analysis, distractor_analysis, why_good_distractor, is_good_question (boolean), improvement_suggestion"""
    
    max_retries = 3
    last_error = ""
    
    for attempt in range(max_retries):
        if attempt > 0:
            time.sleep(attempt * 2)
            
        try:
            response = client.chat.completions.create(
                model=model_id,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message}
                ],
                temperature=0.2,
                max_tokens=2000
            )
            
            raw_text = response.choices[0].message.content
            # Clean JSON from markdown code blocks
            cleaned = re.sub(r'^```(?:json)?\s*|```$', '', raw_text, flags=re.MULTILINE | re.DOTALL).strip()
            start_brace = cleaned.find('{')
            end_brace = cleaned.rfind('}')
            if start_brace != -1 and end_brace > start_brace:
                cleaned = cleaned[start_brace:end_brace+1]
            analysis = json.loads(cleaned)
            return sanitize_analysis(analysis)
            
        except Exception as e:
            last_error = str(e)
            if "429" in str(e):
                 time.sleep(5)

    return create_error_response(f"OpenRouter Error: {last_error}")



def create_error_response(error_message):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á response ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î"""
    return {
        "bloom_level": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏î‡πâ", "reasoning": "AI ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß",
        "difficulty": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÑ‡∏î‡πâ", "curriculum_standard": "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏î‡πâ",
        "correct_option": "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏", "correct_option_analysis": "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏",
        "distractor_analysis": "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏", "why_good_distractor": "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏",
        "is_good_question": False, 
        "improvement_suggestion": f"**‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î**: {error_message}"
    }


def sanitize_analysis(analysis):
    """‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å AI (Robust)"""
    required_keys = [
        "bloom_level", "reasoning", "difficulty", "curriculum_standard",
        "correct_option", "correct_option_analysis", "distractor_analysis",
        "why_good_distractor", "is_good_question", "improvement_suggestion"
    ]
    
    result = {}
    missing_keys = []
    
    for key in required_keys:
        val = analysis.get(key)
        
        # Robust Boolean Conversion
        if key == "is_good_question":
            if isinstance(val, bool):
                result[key] = val
            elif isinstance(val, str):
                result[key] = val.strip().lower() in ['true', 'yes', '1', 'correct', '‡∏à‡∏£‡∏¥‡∏á', '‡πÉ‡∏ä‡πà']
            else:
                result[key] = False
        else:
            # Robust String Conversion
            result[key] = str(val).strip() if val not in [None, "", "null"] else "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏"
            
        if key not in analysis:
            missing_keys.append(key)

    # Specific Default Values for missing keys
    if "improvement_suggestion" not in result or result["improvement_suggestion"] == "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏":
        result["improvement_suggestion"] = "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°"

    return result



def analyze_question(question_text, question_id=1):
    """Wrapper function - ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å provider ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å"""
    provider = st.session_state.get('selected_provider', DEFAULT_PROVIDER)
    
    if provider == "Gemini (Google)":
        return analyze_with_gemini(question_text, question_id)
    elif provider == "Groq (‡∏ü‡∏£‡∏µ+‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å)":
        return analyze_with_groq(question_text, question_id)
    elif provider == "OpenRouter (‡∏´‡∏•‡∏≤‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ü‡∏£‡∏µ)":
        return analyze_with_openrouter(question_text, question_id)
    else:
        return analyze_with_gemini(question_text, question_id)


# ===== EXPORT FUNCTIONS =====
def export_to_excel(analysis_results, filename="exam_analysis.xlsx"):
    """Export ‡∏ú‡∏•‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏õ‡πá‡∏ô Excel"""
    if not EXCEL_AVAILABLE:
        return None
    
    wb = Workbook()
    ws = wb.active
    ws.title = "‡∏ú‡∏•‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö"
    
    # Header styling
    header_fill = PatternFill(start_color="18181B", end_color="18181B", fill_type="solid")
    header_font = Font(color="FFFFFF", bold=True)
    
    headers = ["‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà", "‡∏£‡∏∞‡∏î‡∏±‡∏ö Bloom", "‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å", "‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û", "‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô", "‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö", "‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞"]
    for col, header in enumerate(headers, 1):
        cell = ws.cell(row=1, column=col, value=header)
        cell.fill = header_fill
        cell.font = header_font
        cell.alignment = Alignment(horizontal="center")
    
    # Data rows
    for idx, item in enumerate(analysis_results, 1):
        ws.cell(row=idx+1, column=1, value=idx)
        ws.cell(row=idx+1, column=2, value=item.get('bloom_level', 'N/A'))
        ws.cell(row=idx+1, column=3, value=item.get('difficulty', 'N/A'))
        ws.cell(row=idx+1, column=4, value="‡∏î‡∏µ" if item.get('is_good_question') else "‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á")
        ws.cell(row=idx+1, column=5, value=item.get('curriculum_standard', 'N/A'))
        ws.cell(row=idx+1, column=6, value=item.get('correct_option', 'N/A'))
        ws.cell(row=idx+1, column=7, value=item.get('improvement_suggestion', 'N/A'))
    
    # Auto-adjust column widths
    for col in ws.columns:
        max_length = max(len(str(cell.value or "")) for cell in col)
        ws.column_dimensions[col[0].column_letter].width = min(max_length + 2, 50)
    
    output = io.BytesIO()
    wb.save(output)
    output.seek(0)
    return output


def extract_text_from_docx(file):
    """‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå DOCX (‡∏≠‡πà‡∏≤‡∏ô‡∏û‡∏≤‡∏£‡∏≤‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏•‡∏∞‡∏ï‡∏≤‡∏£‡∏≤‡∏á)"""
    if not DOCX_AVAILABLE:
        return None
    try:
        doc = Document(file)
        full_text = []
        
        # ‡∏≠‡πà‡∏≤‡∏ô paragraphs
        for para in doc.paragraphs:
            if para.text.strip():
                full_text.append(para.text)
                
        # ‡∏≠‡πà‡∏≤‡∏ô tables (‡∏°‡∏±‡∏Å‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö)
        for table in doc.tables:
            for row in table.rows:
                row_text = []
                for cell in row.cells:
                    if cell.text.strip():
                        row_text.append(cell.text.strip())
                if row_text:
                    full_text.append(" ".join(row_text))
                    
        return "\n".join(full_text)
    except Exception as e:
        return None



def generate_exam_with_ai(subject, bloom_level, num_questions, difficulty="‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á"):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡πÉ‡∏´‡∏°‡πà‡∏î‡πâ‡∏ß‡∏¢ AI"""
    provider = st.session_state.get('selected_provider', DEFAULT_PROVIDER)
    
    prompt = f"""‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏ô‡∏±‡∏¢ 4 ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô {num_questions} ‡∏Ç‡πâ‡∏≠
‡∏ß‡∏¥‡∏ä‡∏≤: {subject}
Level: {bloom_level}
‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å: {difficulty}

‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ç‡πâ‡∏≠ ‡πÉ‡∏´‡πâ‡∏°‡∏µ:
1. ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
2. ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å ‡∏Å. ‡∏Ç. ‡∏Ñ. ‡∏á.
3. ‡πÄ‡∏â‡∏•‡∏¢
4. ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö

‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô JSON array ‡∏ó‡∏µ‡πà‡∏°‡∏µ keys: question, options (array), answer, explanation"""
    
    try:
        if provider == "Gemini (Google)" and GEMINI_AVAILABLE:
            model = genai.GenerativeModel("gemini-2.0-flash")
            response = model.generate_content(prompt)
            raw_text = response.text
        elif provider == "Groq (‡∏ü‡∏£‡∏µ+‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å)" and GROQ_AVAILABLE:
            from groq import Groq
            client = Groq(api_key=GROQ_API_KEY)
            response = client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7
            )
            raw_text = response.choices[0].message.content
        elif provider == "OpenRouter (‡∏´‡∏•‡∏≤‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ü‡∏£‡∏µ)" and OPENROUTER_AVAILABLE:
            import openai
            client = openai.OpenAI(base_url="https://openrouter.ai/api/v1", api_key=OPENROUTER_API_KEY)
            response = client.chat.completions.create(
                model="meta-llama/llama-3.2-3b-instruct:free",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7
            )
            raw_text = response.choices[0].message.content
        else:
            return None, "‡πÑ‡∏°‡πà‡∏°‡∏µ API Key ‡∏ó‡∏µ‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô"
        
        # Parse JSON from response
        cleaned = re.sub(r'^```(?:json)?\s*|```$', '', raw_text, flags=re.MULTILINE | re.DOTALL).strip()
        start = cleaned.find('[')
        end = cleaned.rfind(']') + 1
        if start != -1 and end > start:
            exams = json.loads(cleaned[start:end])
            return exams, None
        return None, "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ parse JSON ‡πÑ‡∏î‡πâ"
    except Exception as e:
        return None, str(e)


def improve_question_with_ai(question_text, suggestion):
    """‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏ï‡∏≤‡∏°‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ AI"""
    provider = st.session_state.get('selected_provider', DEFAULT_PROVIDER)
    
    prompt = f"""‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡πÄ‡∏î‡∏¥‡∏°:
{question_text}

‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á:
{suggestion}

‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞ ‡πÇ‡∏î‡∏¢‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å‡πÑ‡∏ß‡πâ ‡πÅ‡∏ï‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏à‡∏∏‡∏î‡∏ö‡∏Å‡∏û‡∏£‡πà‡∏≠‡∏á
‡∏ï‡∏≠‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö:
- ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
- ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å ‡∏Å. ‡∏Ç. ‡∏Ñ. ‡∏á.
- (‡πÄ‡∏â‡∏•‡∏¢: ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á)"""
    
    try:
        if provider == "Gemini (Google)" and GEMINI_AVAILABLE:
            model = genai.GenerativeModel("gemini-2.0-flash")
            response = model.generate_content(prompt)
            return response.text, None
        elif provider == "Groq (‡∏ü‡∏£‡∏µ+‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å)" and GROQ_AVAILABLE:
            from groq import Groq
            client = Groq(api_key=GROQ_API_KEY)
            response = client.chat.completions.create(
                model="llama-3.3-70b-versatile",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.5
            )
            return response.choices[0].message.content, None
        elif provider == "OpenRouter (‡∏´‡∏•‡∏≤‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ü‡∏£‡∏µ)" and OPENROUTER_AVAILABLE:
            import openai
            client = openai.OpenAI(base_url="https://openrouter.ai/api/v1", api_key=OPENROUTER_API_KEY)
            response = client.chat.completions.create(
                model="meta-llama/llama-3.2-3b-instruct:free",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.5
            )
            return response.choices[0].message.content, None
        return None, "‡πÑ‡∏°‡πà‡∏°‡∏µ API Key ‡∏ó‡∏µ‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô"
    except Exception as e:
        return None, str(e)


def save_analysis_history(filename, results, summary):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå"""
    history_file = "analysis_history.json"
    history = []
    
    if os.path.exists(history_file):
        try:
            with open(history_file, 'r', encoding='utf-8') as f:
                history = json.load(f)
        except:
            history = []
    
    entry = {
        "timestamp": datetime.now().isoformat(),
        "filename": filename,
        "total_questions": len(results),
        "good_questions": sum(1 for r in results if r.get('is_good_question')),
        "summary": summary
    }
    history.insert(0, entry)  # Add to beginning
    history = history[:20]  # Keep only last 20
    
    with open(history_file, 'w', encoding='utf-8') as f:
        json.dump(history, f, ensure_ascii=False, indent=2)


def load_analysis_history():
    """‡πÇ‡∏´‡∏•‡∏î‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå"""
    history_file = "analysis_history.json"
    if os.path.exists(history_file):
        try:
            with open(history_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return []
    return []


def extract_text_from_docx(file):
    """‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå DOCX"""
    if not DOCX_AVAILABLE:
        return None
    try:
        doc = DocxDocument(file)
        full_text = []
        for para in doc.paragraphs:
            full_text.append(para.text)
        return '\n'.join(full_text)
    except Exception as e:
        return None


def check_bloom_criteria(analysis_results):
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏£‡∏∞‡∏î‡∏±‡∏ö Bloom ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà."""
    total = len(analysis_results)
    if total == 0: 
        return {"pass": False, "reason": "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö", "percentages": {}, "raw_counts": {}, "valid_total": 0} 

    # Standardized Level Names for grouping
    LEVEL_GROUPS = {
        "Remember": 0, "Understand": 0, "Apply": 0, 
        "Analyze": 0, "Evaluate": 0, "Create": 0, "Unknown": 0
    }
    
    for item in analysis_results:
        level = str(item.get("bloom_level", "")).strip().lower()
        if "remember" in level: LEVEL_GROUPS["Remember"] += 1
        elif "understand" in level: LEVEL_GROUPS["Understand"] += 1
        elif "apply" in level: LEVEL_GROUPS["Apply"] += 1
        elif "analyze" in level: LEVEL_GROUPS["Analyze"] += 1
        elif "evaluate" in level: LEVEL_GROUPS["Evaluate"] += 1
        elif "create" in level: LEVEL_GROUPS["Create"] += 1
        else: LEVEL_GROUPS["Unknown"] += 1

    apply_analyze_count = LEVEL_GROUPS["Apply"] + LEVEL_GROUPS["Analyze"]
    remember_understand_count = LEVEL_GROUPS["Remember"] + LEVEL_GROUPS["Understand"]
    evaluate_create_count = LEVEL_GROUPS["Evaluate"] + LEVEL_GROUPS["Create"]
    valid_total = total - LEVEL_GROUPS["Unknown"] # ‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡∏£‡∏∞‡∏î‡∏±‡∏ö Bloom ‡πÑ‡∏î‡πâ

    if valid_total == 0:
        return {"pass": False, "reason": "‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢", "percentages": {}, "raw_counts": LEVEL_GROUPS, "valid_total": 0} 
        
    percent = {
        "Remember/Understand": round((remember_understand_count / valid_total) * 100, 1) if valid_total > 0 else 0,
        "Apply/Analyze": round((apply_analyze_count / valid_total) * 100, 1) if valid_total > 0 else 0,
        "Evaluate/Create": round((evaluate_create_count / valid_total) * 100, 1) if valid_total > 0 else 0
    }

    # ‡πÄ‡∏Å‡∏ì‡∏ë‡πå (‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: ‡∏ï‡πà‡∏≥ ‚â§ 40%, ‡∏Å‡∏•‡∏≤‡∏á ‚â• 50%, ‡∏™‡∏π‡∏á ‚â• 10%)
    pass_r_u = percent["Remember/Understand"] <= 40
    pass_a_a = percent["Apply/Analyze"] >= 50
    pass_e_c = percent["Evaluate/Create"] >= 10
    passed = pass_r_u and pass_a_a and pass_e_c

    reason = "‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö **‡∏ú‡πà‡∏≤‡∏ô** ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î (Bloom)" if passed else "‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö **‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô** ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î (Bloom)"

    return {
        "pass": passed,
        "reason": reason,
        "percentages": percent,
        "raw_counts": LEVEL_GROUPS,
        "valid_total": valid_total 
    }

def create_analysis_report(all_analysis, bloom_check):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏•‡∏∞ DataFrame ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•"""
    total_questions = len(all_analysis)
    
    failed_analysis = sum(1 for a in all_analysis if "QUOTA EXCEEDED" in a.get('improvement_suggestion', '') or "‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå" in a.get('improvement_suggestion', ''))
    
    successfully_analyzed_questions = total_questions - failed_analysis
    good_questions = sum(1 for a in all_analysis if a.get('is_good_question') is True and a.get('bloom_level') != "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏î‡πâ")

    summary_data = {
        "‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°": {
            "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î": f"{total_questions} ‡∏Ç‡πâ‡∏≠",
            "‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à": f"{successfully_analyzed_questions} ‡∏Ç‡πâ‡∏≠",
            "‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö **‡∏î‡∏µ** (‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢)": f"{good_questions} ‡∏Ç‡πâ‡∏≠ ({round((good_questions/successfully_analyzed_questions)*100, 1) if successfully_analyzed_questions > 0 else 0}%)",
            "‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö **‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á**": f"{successfully_analyzed_questions - good_questions} ‡∏Ç‡πâ‡∏≠ ({round(((successfully_analyzed_questions - good_questions)/successfully_analyzed_questions)*100, 1) if successfully_analyzed_questions > 0 else 0}%)"
        },
        "‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î": {
            "‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°": bloom_check['reason'],
            "‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡∏ï‡πà‡∏≥ (‡∏à‡∏≥/‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à) (‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ ‚â§ 40%)": f"{bloom_check['percentages'].get('Remember/Understand', 0)}%",
            "‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡∏Å‡∏•‡∏≤‡∏á (‡πÉ‡∏ä‡πâ/‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå) (‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ ‚â• 50%)": f"{bloom_check['percentages'].get('Apply/Analyze', 0)}%", 
            "‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡∏™‡∏π‡∏á (‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô/‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏£‡∏Ñ‡πå) (‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ ‚â• 10%)": f"{bloom_check['percentages'].get('Evaluate/Create', 0)}%",
            "‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ": f"{bloom_check['raw_counts'].get('Unknown', 0)} ‡∏Ç‡πâ‡∏≠",
            "raw_counts": bloom_check['raw_counts'],
            "valid_total": bloom_check.get('valid_total', 0)
        }
    }
    
    df_data = []
    for i, item in enumerate(all_analysis):
        is_good = "&#9989; ‡∏î‡∏µ" if item.get('is_good_question') is True and item.get('bloom_level') != "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏î‡πâ" else "&#10060; ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á/‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß"
        df_data.append({
            '‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà': i + 1,
            '‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö': is_good,
            '‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î': item.get('bloom_level', '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏'),
            '‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£': item.get('curriculum_standard', '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏'),
            '‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö': item.get('correct_option', '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏'),
            '‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡πÇ‡∏î‡∏¢‡∏¢‡πà‡∏≠': item.get('reasoning', '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•'),
            '‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞': item.get('improvement_suggestion', '‡πÑ‡∏°‡πà‡∏°‡∏µ'),
            '‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏ï‡πá‡∏°': item.get('question_text', '')
        })
    df = pd.DataFrame(df_data)
    return summary_data, df


def toggle_language():
    if st.session_state.language == 'th':
        st.session_state.language = 'en'
    else:
        st.session_state.language = 'th'

# --- 4. Main App Function (UI) ---

def run_app():
    # üé® ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠
    st.set_page_config(
        page_title="‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ (Gemini AI)",
        page_icon="üìù", 
        layout="wide",
        initial_sidebar_state="auto", 
        menu_items=None
    )
    
    # üé® Shadcn/Tailwind CSS
    st.markdown(SHADCN_CSS, unsafe_allow_html=True)
    
    # Language toggle function

    
    # Modern Minimal Header (Dynamic)
    render_top_navigation()
    st.markdown('---')
    st.markdown(f"""
    <div style="text-align: center; padding: 1.5rem 1rem 2rem 1rem; margin-bottom: 0.5rem;">
        <h1 style="font-size: 2.4rem; font-weight: 700; background: linear-gradient(135deg, #6366f1 0%, #8b5cf6 50%, #a855f7 100%); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text; margin-bottom: 0.75rem;">
            {t('app_title')}
        </h1>
        <p style="font-size: 1.1rem; color: #4b5563; max-width: 600px; margin: 0 auto; line-height: 1.6;">
            {t('app_subtitle')}
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown("<hr>", unsafe_allow_html=True) 

    with st.sidebar:
        render_history_sidebar_v2()




    # --- Step 1: Upload ---
    
    # --- Custom Prompt (Main) ---
    st.markdown("---")
    with st.expander(t('custom_prompt_title'), expanded=False):
        st.markdown(f"**{t('custom_prompt_label')}**")
        custom_prompt_input = st.text_area(
            "Custom Prompt",
            value=st.session_state.custom_prompt,
            height=150,
            placeholder=t('custom_prompt_placeholder'),
            key='custom_prompt_main',
            label_visibility="collapsed"
        )
        if custom_prompt_input != st.session_state.custom_prompt:
            st.session_state.custom_prompt = custom_prompt_input
            st.session_state.analysis_results = None
        
        col_status1, col_status2 = st.columns([1, 1])
        with col_status1:
            if st.session_state.custom_prompt.strip():
                st.success(t('custom_prompt_active'))
            else:
                st.info(t('custom_prompt_default'))

    st.markdown("---")
    st.header(t('step1_title'))
    with st.container(border=True):
        st.markdown(f"**{t('file_uploader_label')}**")
        uploaded_file = st.file_uploader(
            t('file_uploader_label'), 
            type=['pdf', 'txt', 'docx'], 
            accept_multiple_files=False, 
            key='file_uploader_widget', 
            label_visibility="collapsed"
        )
        st.caption(t('file_tip'))

        if uploaded_file is not None:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            if uploaded_file.name != st.session_state.last_uploaded_file_name:
                st.session_state.analysis_results = None
                st.session_state.last_uploaded_file_name = uploaded_file.name
                st.session_state.question_texts = None
                
            if st.session_state.question_texts is None:
                file_extension = uploaded_file.name.split('.')[-1].lower()
                
                # Custom Loading UI ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö
                status_container = st.empty()
                status_container.info(f"{t('reading_file')}\n\n{t('from_file')} **{uploaded_file.name}**...")
                
                with st.spinner(t('extracting')):
                    try:
                        if file_extension == 'pdf':
                            with io.BytesIO(uploaded_file.getvalue()) as open_pdf_file:
                                pdf_reader = PdfReader(open_pdf_file)
                                raw_text = extract_text_from_pdf(pdf_reader)
                        elif file_extension == 'docx':
                            raw_text = extract_text_from_docx(uploaded_file)
                            if raw_text is None:
                                raise ValueError("‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå DOCX ‡πÑ‡∏î‡πâ ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á python-docx")
                        elif file_extension == 'txt':
                            raw_text = uploaded_file.getvalue().decode("utf-8")
                        
                        question_texts = extract_questions(raw_text)
                        st.session_state.question_texts = question_texts
                        
                        status_container.empty() # ‡∏•‡πâ‡∏≤‡∏á Custom Loading
                        
                        if not question_texts:
                            st.error(t('no_questions_found'))
                            st.info(t('file_tip'))
                            return 
                        
                    except Exception as e:
                        status_container.empty() # ‡∏•‡πâ‡∏≤‡∏á Custom Loading
                        st.error(f"{t('file_read_error')} {e}")
                        return 
                
                # Rerun ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏™‡∏Å‡∏±‡∏î‡πÄ‡∏™‡∏£‡πá‡∏à ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
                st.rerun() 

            question_texts = st.session_state.question_texts
            if question_texts:
                st.success(t('extracted_questions').format(count=len(question_texts), filename=uploaded_file.name))
            


    # --- Step 2: Start Analysis ---
    st.markdown("---")
    st.header(t('step2_title'))
    
    # ‡πÉ‡∏ä‡πâ Callback function ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Rerun ‡∏ã‡πâ‡∏≥‡∏ã‡πâ‡∏≠‡∏ô)
    def start_analysis_callback():
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö API ‡∏ï‡∏≤‡∏° provider ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
        provider = st.session_state.get('selected_provider', DEFAULT_PROVIDER)
        provider_available = (
            (provider == "Gemini (Google)" and GEMINI_AVAILABLE) or
            (provider == "Groq (‡∏ü‡∏£‡∏µ+‡πÄ‡∏£‡πá‡∏ß‡∏°‡∏≤‡∏Å)" and GROQ_AVAILABLE) or
            (provider == "OpenRouter (‡∏´‡∏•‡∏≤‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ü‡∏£‡∏µ)" and OPENROUTER_AVAILABLE)
        )
        
        if not provider_available:
            st.error(f"&#10060; ‡πÑ‡∏°‡πà‡∏û‡∏ö API Key ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö {provider} - ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô .env")
            return
            
        question_texts = st.session_state.question_texts
        analysis_results = []
        
        # ‡πÉ‡∏ä‡πâ st.status ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏ß‡∏°‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        with st.status(t('starting_analysis'), expanded=True) as status_box:
            
            # ‡∏î‡∏∂‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
            provider_models = AI_PROVIDERS.get(provider, {}).get("models", {})
            current_model = provider_models.get(st.session_state.selected_model, "unknown")
            st.write(f"‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå {len(question_texts)} ‡∏Ç‡πâ‡∏≠ ‡∏î‡πâ‡∏ß‡∏¢ `{provider}` > `{current_model}`")
            progress_bar = st.progress(0, text=t('analysis_progress').format(current=0, total=len(question_texts)))
            
            for i, q_text in enumerate(question_texts):
                st.write(t('analyzing_question').format(num=i+1))
                analysis = analyze_question(q_text, question_id=i+1)  # ‡πÉ‡∏ä‡πâ wrapper function
                if "**‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î" in analysis.get('improvement_suggestion', ''):
                     st.error(f"Error analyzing question {i+1}: {analysis.get('improvement_suggestion')}")

                analysis["question_text"] = q_text 
                analysis_results.append(analysis)
                
                progress_percent = (i + 1) / len(question_texts)
                progress_bar.progress(progress_percent, text=t('analysis_progress').format(current=i+1, total=len(question_texts)))
                
                # ‡πÄ‡∏û‡∏¥‡πà‡∏° delay ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á requests ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á rate limit
                if i < len(question_texts) - 1:  # ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≠‡∏´‡∏•‡∏±‡∏á‡∏Ç‡πâ‡∏≠‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
                    time.sleep(2)  # ‡∏£‡∏≠ 2 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ request
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏•‡∏á‡πÉ‡∏ô session state
            st.session_state.analysis_results = analysis_results
            
            # ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞
            status_box.update(label=t('analysis_complete'), state="complete", expanded=False)


    if st.session_state.question_texts and st.button(
        t('start_analysis_btn'), 
        type="primary", 
        use_container_width=True,
        on_click=start_analysis_callback 
    ):
        pass


    # --- Step 3: Report ---
    if st.session_state.analysis_results:
        st.divider()
        st.header(t('step3_title'))

        all_analysis = st.session_state.analysis_results
        successful_analysis = [a for a in all_analysis if a.get('bloom_level') != "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏î‡πâ"]
        bloom_check = check_bloom_criteria(successful_analysis)
        summary_data, df = create_analysis_report(all_analysis, bloom_check)
        
        # ‡∏î‡∏∂‡∏á valid_total ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á (‡πÅ‡∏Å‡πâ NameError)
        valid_total = summary_data["‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î"].get("valid_total", 0)
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
        save_analysis_history(
            st.session_state.get('last_uploaded_file_name', 'unknown'),
            all_analysis,
            summary_data
        )

        # ‡πÉ‡∏ä‡πâ Tabs ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏î‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô - ‡πÄ‡∏û‡∏¥‡πà‡∏° Export tab
        tab_summary, tab_details, tab_export = st.tabs([
            t('tab_summary'), 
            t('tab_details'),
            "&#128229; Export ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô"
        ])

        # --- Tab: Summary ---
        with tab_summary:
            st.subheader(t('summary_title'))
            stats = summary_data["‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°"]
            col1, col2, col3, col4 = st.columns(4) 

            # Helper function to extract percent value
            def get_percent_delta(text):
                try: 
                    return text.split('(')[1].strip('%)') 
                except IndexError: 
                    return "0.0%"

            # Good Questions Metric
            good_count_str = stats["‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö **‡∏î‡∏µ** (‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢)"].split(' ')[0]
            good_percent_str = get_percent_delta(stats["‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö **‡∏î‡∏µ** (‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢)"])
            col1.metric(t('good_questions'), good_count_str, delta=f"{good_percent_str}%", delta_color="normal")
            
            # To Improve Metric
            improve_count_str = stats["‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö **‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á**"].split(' ')[0]
            improve_percent_str = get_percent_delta(stats["‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö **‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á**"])
            col2.metric(t('needs_improvement'), improve_count_str, delta=f"{improve_percent_str}%", delta_color="inverse")
            
            # Total Questions 
            col3.metric(t('total_questions'), stats["‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"].split(' ')[0])
            
            # Successfully Analyzed
            col4.metric(t('analyzed_success'), stats["‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à"].split(' ')[0])
            
            st.markdown("---")
            st.subheader(t('bloom_criteria_title'))
            bloom_stats = summary_data["‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î"]
            
            if bloom_check['pass']:
                st.success(f"**üéâ {bloom_stats['‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°']}**")
            else:
                st.warning(f"**&#10060; {bloom_stats['‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°']}**")
                
            col_b1, col_b2, col_b3 = st.columns(3)
            col_b1.metric(t('bloom_low'), bloom_stats["‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡∏ï‡πà‡∏≥ (‡∏à‡∏≥/‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à) (‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ ‚â§ 40%)"], delta=f"{t('target')} ‚â§ 40%")
            col_b2.metric(t('bloom_mid'), bloom_stats["‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡∏Å‡∏•‡∏≤‡∏á (‡πÉ‡∏ä‡πâ/‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå) (‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ ‚â• 50%)"], delta=f"{t('target')} ‚â• 50%")
            col_b3.metric(t('bloom_high'), bloom_stats["‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡∏™‡∏π‡∏á (‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô/‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏£‡∏Ñ‡πå) (‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ ‚â• 10%)"], delta=f"{t('target')} ‚â• 10%")
            
            st.markdown(f"{t('unidentified_bloom')} {bloom_stats['‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ']}")
            
            
            # --- ‡∏™‡∏£‡πâ‡∏≤‡∏á Pie Chart ‡πÅ‡∏•‡∏∞ ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ ---
            st.markdown("---")
            st.subheader(t('bloom_distribution'))
            
            col_chart, col_table = st.columns([1, 1.2]) 

            # 1. Pie Chart 
            with col_chart:
                bloom_counts = bloom_stats['raw_counts']
                # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Pie Chart (‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° Unknown)
                chart_data_raw = {
                    '‡∏£‡∏∞‡∏î‡∏±‡∏ö Bloom': list(bloom_counts.keys())[:-1],
                    '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠': list(bloom_counts.values())[:-1],
                    '‡∏™‡∏µ': [get_bloom_color(level) for level in list(bloom_counts.keys())[:-1]]
                }
                chart_df = pd.DataFrame(chart_data_raw)
                
                if not chart_df.empty and chart_df['‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠'].sum() > 0:
                    base = alt.Chart(chart_df).encode(
                        theta=alt.Theta("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠", stack=True)
                    )
                    
                    pie = base.mark_arc(outerRadius=120).encode(
                        color=alt.Color("‡∏£‡∏∞‡∏î‡∏±‡∏ö Bloom", scale=alt.Scale(domain=chart_df['‡∏£‡∏∞‡∏î‡∏±‡∏ö Bloom'].tolist(), range=chart_df['‡∏™‡∏µ'].tolist())),
                        order=alt.Order("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠", sort="descending"),
                        tooltip=["‡∏£‡∏∞‡∏î‡∏±‡∏ö Bloom", "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠"]
                    )
                    
                    st.altair_chart(pie, use_container_width=True)
                else:
                    st.warning("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏∞‡∏î‡∏±‡∏ö Bloom ‡πÑ‡∏î‡πâ")
            
            # 2. ‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û/‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö Bloom
            with col_table:
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡∏™‡∏£‡∏∏‡∏õ
                summary_table_data = []
                for level, count in bloom_stats['raw_counts'].items():
                    if level == 'Unknown': continue 
                    
                    level_items = [a for a in all_analysis if level.lower() in a.get('bloom_level', '').lower()]
                    good_count = sum(1 for a in level_items if a.get('is_good_question') is True)
                    
                    percent_text = f"{round((count / valid_total) * 100, 1) if valid_total > 0 else 0}%"
                    
                    summary_table_data.append({
                        '‡∏£‡∏∞‡∏î‡∏±‡∏ö Bloom': level,
                        '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠': count,
                        '‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô': percent_text,
                        '‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏î‡∏µ': good_count,
                        '‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á': count - good_count
                    })
                
                summary_table_df = pd.DataFrame(summary_table_data)
                
                st.markdown("**‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏ï‡∏≤‡∏°‡∏£‡∏∞‡∏î‡∏±‡∏ö Bloom**")
                st.dataframe(
                    summary_table_df, 
                    hide_index=True, 
                    use_container_width=True,
                    column_config={
                        '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠': st.column_config.NumberColumn(format="%d ‡∏Ç‡πâ‡∏≠"),
                        '‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏î‡∏µ': st.column_config.NumberColumn(format="%d ‡∏Ç‡πâ‡∏≠"),
                        '‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á': st.column_config.NumberColumn(format="%d ‡∏Ç‡πâ‡∏≠"),
                    }
                )

        # --- Tab: Details ---
        with tab_details:
            st.subheader(t('details_title'))
            
            # 1. ‡πÅ‡∏™‡∏î‡∏á DataFrame ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡πà‡∏≠‡∏ô
            st.dataframe(
                df[[
                    '‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà', '‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö', '‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î', 
                    '‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£', '‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö', '‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡πÇ‡∏î‡∏¢‡∏¢‡πà‡∏≠', 
                    '‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞'
                ]],
                column_config={
                    "‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö": st.column_config.Column("‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö", width="small"),
                    "‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î": st.column_config.Column("‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î", width="small"),
                    "‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡πÇ‡∏î‡∏¢‡∏¢‡πà‡∏≠": st.column_config.Column("‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡πÇ‡∏î‡∏¢‡∏¢‡πà‡∏≠", width="medium"),
                },
                use_container_width=True,
                hide_index=True
            )
            
            # 2. Loop ‡∏™‡∏£‡πâ‡∏≤‡∏á expander ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å‡∏Ç‡πâ‡∏≠
            st.markdown("---")
            st.markdown(t('click_detail'))
            
            for q_index, item in enumerate(all_analysis):
                quality_status = t('good') if item.get('is_good_question') is True and item.get('bloom_level') != "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏∞‡∏ö‡∏∏‡πÑ‡∏î‡πâ" else t('improve')
                expander_title = f"**{t('question_num')} {q_index+1}** | {quality_status} | {t('bloom_level')}: **{item.get('bloom_level', '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏')}**"
                
                # ‡πÉ‡∏ä‡πâ st.expander ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
                with st.expander(expander_title):
                    
                    st.markdown(t('full_question'))
                    st.code(item.get('question_text', 'N/A'), language='markdown')
                    
                    st.markdown("---")

                    # ‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡πÅ‡∏ö‡∏ö‡∏ä‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏™‡∏µ‡∏™‡∏±‡∏ô‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏° 
                    bloom_color = get_bloom_color(item.get('bloom_level', '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏'))
                    text_color = get_text_color_for_bloom(item.get('bloom_level', '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏'))
                    
                    col_det1, col_det2, col_det3 = st.columns([1, 1, 1]) 

                    with col_det1:
                        # ‡πÅ‡∏™‡∏î‡∏á Bloom Level
                        st.markdown(
                            f"""
                            <div style='background-color:{bloom_color}; color:{text_color}; padding: 10px; border-radius: 5px; text-align: center; margin-bottom: 10px;'>
                                <strong>&#128161; ‡∏£‡∏∞‡∏î‡∏±‡∏ö Bloom:</strong> {item.get('bloom_level', 'N/A')}
                            </div>
                            """, unsafe_allow_html=True
                        )
                    
                    with col_det2:
                         # ‡πÅ‡∏™‡∏î‡∏á Difficulty (‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏™‡∏µ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£)
                        difficulty_level = item.get('difficulty', '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏')
                        difficulty_color = {"‡∏á‡πà‡∏≤‡∏¢": "#008000", "‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á": "#FFA500", "‡∏¢‡∏≤‡∏Å": "#FF4500"}.get(difficulty_level, "#808080")
                        
                        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏™‡∏µ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡∏ï‡∏≤‡∏°‡∏û‡∏∑‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏á
                        if difficulty_level in ["‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á"]: 
                            difficulty_text_color = "white" 
                        else:
                            difficulty_text_color = "white"
                        
                        st.markdown(
                            f"""
                            <div style='background-color:{difficulty_color}; color:{difficulty_text_color}; padding: 10px; border-radius: 5px; text-align: center; margin-bottom: 10px;'>
                                <strong>{t('difficulty')}</strong> {difficulty_level}
                            </div>
                            """, unsafe_allow_html=True
                        )

                    with col_det3:
                        # ‡πÅ‡∏™‡∏î‡∏á Correct Option
                        st.markdown(
                            f"""
                            <div style='background-color:#0077B6; color:white; padding: 10px; border-radius: 5px; text-align: center; margin-bottom: 10px;'>
                                <strong>{t('correct_answer')}</strong> {item.get('correct_option', 'N/A')}
                            </div>
                            """, unsafe_allow_html=True
                        )

                    # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏Å 
                    st.markdown(f"{t('curriculum_indicator')} `{item.get('curriculum_standard', 'N/A')}`")
                    st.markdown(f"{t('bloom_reason')} {item.get('reasoning', 'N/A')}")
                    
                    st.divider()
                    st.subheader(t('answer_analysis_title'))
                    st.markdown(f"{t('correct_analysis')} {item.get('correct_option_analysis', 'N/A')}")
                    st.markdown(f"{t('distractor_analysis')} {item.get('distractor_analysis', 'N/A')}")
                    st.markdown(f"{t('why_good_distractor')} {item.get('why_good_distractor', 'N/A')}")
                    st.warning(f"{t('improvement_suggestion')} {item.get('improvement_suggestion', 'N/A')}")
                    
                    # ‡∏õ‡∏∏‡πà‡∏°‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢ AI
                    if st.button(f"&#10024; ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏ó‡∏µ‡πà {q_index+1}", key=f"improve_{q_index}"):
                        with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö..."):
                            improved, err = improve_question_with_ai(
                                item.get('question_text', ''),
                                item.get('improvement_suggestion', '')
                            )
                            if improved:
                                st.success("&#9989; ‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß:")
                                st.markdown(improved)
                            else:
                                st.error(f"&#10060; ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {err}")
                
                st.divider() 


        # --- Tab: Export ---
        with tab_export:
            st.subheader("&#128229; ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå")
            
            col_excel, col_info = st.columns([1, 2])
            
            with col_excel:
                if EXCEL_AVAILABLE:
                    excel_data = export_to_excel(all_analysis)
                    if excel_data:
                        st.download_button(
                            label="üìä ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Excel",
                            data=excel_data,
                            file_name=f"exam_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                            use_container_width=True
                        )
                else:
                    st.warning("‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á openpyxl ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ô‡∏µ‡πâ")
            
            with col_info:
                st.info(f"""
                **‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏à‡∏∞ Export:**
                - ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö: {len(all_analysis)} ‡∏Ç‡πâ‡∏≠
                - ‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏î‡∏µ: {sum(1 for a in all_analysis if a.get('is_good_question'))} ‡∏Ç‡πâ‡∏≠
                - ‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á: {len(all_analysis) - sum(1 for a in all_analysis if a.get('is_good_question'))} ‡∏Ç‡πâ‡∏≠
                """)

    # --- Section: Generate New Exam ---
    st.divider()
    st.header("&#127381; ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡πÉ‡∏´‡∏°‡πà‡∏î‡πâ‡∏ß‡∏¢ AI")
    
    with st.expander("&#128161; ‡∏Ñ‡∏•‡∏¥‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡πÉ‡∏´‡∏°‡πà", expanded=False):
        col_gen1, col_gen2 = st.columns(2)
        
        with col_gen1:
            subject = st.text_input("&#128218; ‡∏ß‡∏¥‡∏ä‡∏≤/‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠", placeholder="‡πÄ‡∏ä‡πà‡∏ô ‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå ‡∏°.3, ‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå ‡∏õ.6")
            bloom_level = st.selectbox(
                "&#129504; ‡∏£‡∏∞‡∏î‡∏±‡∏ö Bloom's Taxonomy",
                ["‡∏à‡∏≥ (Remember)", "‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à (Understand)", "‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ (Apply)", 
                 "‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå (Analyze)", "‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡πà‡∏≤ (Evaluate)", "‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏£‡∏Ñ‡πå (Create)"]
            )
        
        with col_gen2:
            num_questions = st.number_input("üìù ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠", min_value=1, max_value=20, value=5)
            difficulty = st.selectbox("üìä ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏Å", ["‡∏á‡πà‡∏≤‡∏¢", "‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á", "‡∏¢‡∏≤‡∏Å"])
        
        if st.button("&#128640; ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö", type="primary", use_container_width=True):
            if not subject:
                st.warning("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡∏¥‡∏ä‡∏≤/‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠")
            else:
                with st.spinner(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö {num_questions} ‡∏Ç‡πâ‡∏≠..."):
                    exams, err = generate_exam_with_ai(subject, bloom_level, num_questions, difficulty)
                    if exams:
                        st.success(f"&#9989; ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏™‡∏≠‡∏ö‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à {len(exams)} ‡∏Ç‡πâ‡∏≠!")
                        for i, exam in enumerate(exams, 1):
                            with st.expander(f"‡∏Ç‡πâ‡∏≠ {i}: {exam.get('question', 'N/A')[:50]}..."):
                                st.markdown(f"**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:** {exam.get('question', 'N/A')}")
                                st.markdown("**‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å:**")
                                options = exam.get('options', [])
                                for j, opt in enumerate(options):
                                    prefix = ['‡∏Å.', '‡∏Ç.', '‡∏Ñ.', '‡∏á.'][j] if j < 4 else f"{j+1}."
                                    st.markdown(f"   {prefix} {opt}")
                                st.markdown(f"**‡πÄ‡∏â‡∏•‡∏¢:** {exam.get('answer', 'N/A')}")
                                st.markdown(f"**‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:** {exam.get('explanation', 'N/A')}")
                    else:
                        st.error(f"&#10060; ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {err}")

    # --- Section: History ---
    st.divider()
    st.header("&#128220; ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå")
    
    history = load_analysis_history()
    if history:
        for entry in history[:5]:  # ‡πÅ‡∏™‡∏î‡∏á 5 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
            timestamp = entry.get('timestamp', 'N/A')[:10]
            filename = entry.get('filename', 'N/A')
            total = entry.get('total_questions', 0)
            good = entry.get('good_questions', 0)
            st.markdown(f"üìÅ **{filename}** - {timestamp} | {total} ‡∏Ç‡πâ‡∏≠ (‡∏î‡∏µ: {good})")
    else:
        st.info("‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå")


if __name__ == "__main__":
    run_app()